write_excel_csv(playersAllYears, file.path("~","GitHub","ieseDataSciProject","playersAllYears.xlsx"))
write_excel_csv(playersAllYears, file.path("~","GitHub","ieseDataSciProject","playersAllYears.csv"))
source('~/GitHub/ieseDataSciProject/golf.r')
write_excel_csv(playersAllYears, file.path("~","GitHub","ieseDataSciProject","playersAllYears.csv"))
# Reading HTML nodes
golfHTML <- read_html(cleanHTML)
# Identifying errors in player profile links and fixing
cleanHTML <- sapply(read_lines(fileName),str_replace, "(/players/.*) ","\\1-")
#Loading libraries
library("XML")
library("rvest")
library("tidyverse")
library("stringr")
library("ggplot2")
# Initiate variable
playersAllYears <- data.frame(
Year = numeric(),
Rank = numeric(),
ID = factor(),
Name = character(),
Country = factor(),
OfficialMoney = numeric(),
EventsPlayed = numeric()
)
year <- seq(from = 2018, to = 2000, by = -1)
url <- c('http://www.lpga.com/statistics/money/official-money?year=')
for (y in year) {
# Downloading html
fileName <- paste("golf",y,".html")
fileNameClean <- paste("golfClean",y,".html")
path <- file.path("~","GitHub","ieseDataSciProject",fileName)
download.file(paste(url,y), destfile = path)
# Players scraped in table
colTypes <- c("numeric","character","character","numeric")
players <- readHTMLTable(paste(url,y), colClasses = colTypes, stringsAsFactors = FALSE, as.data.frame = TRUE, which = 1)
names(players)[3:4] <- c("OfficialMoney", "EventsPlayed")
str(players)
# Converting official money to numeric
players$`OfficialMoney`<- as.numeric(gsub("\\$|,","",players$`OfficialMoney`))
str(players)
# Identifying errors in player profile links and fixing
cleanHTML <- sapply(read_lines(fileName),str_replace, "(/players/.*) ","\\1-")
write(cleanHTML, fileNameClean)
# Reading HTML nodes
golfHTML <- read_html(fileNameClean)
golfPlayers <- html_nodes(golfHTML, xpath = "//tbody/tr/td[contains(@class, 'table-content left')]/a")
# Identifying missing link attributes
missingProfileLinks <- which(sapply(html_attrs(golfPlayers),is_empty)) -1
# Extracting player ID
golfPlayersURL <- unlist(html_attrs(golfPlayers))
playersID <- matrix(unlist(strsplit(golfPlayersURL,"/")),nrow=length(golfPlayersURL), byrow = TRUE) [,4]
# Adding NAs for missing profiles
for (i in missingProfileLinks){
playersID <- append(playersID, NA, after = i)
}
# Adding player ID to main table
players <- add_column(players,as.factor(playersID), .after = "Rank")
names(players)[2] <- c("ID")
str(players)
# Reading HTML nodes for country flag links
golfPlayersCountries <- html_nodes(golfHTML, xpath = "//tbody/tr/td[contains(@class, 'table-content left')]/a/div")
# Identifying missing link attributes
missingCountryLinks <- which(sapply(lapply(golfPlayersCountries,html_children),is_empty)) -1
# Extracting player country
golfPlayersCountriesURL <- unlist(html_attrs(html_children(golfPlayersCountries)))
playersCountries <- toupper(str_match(golfPlayersCountriesURL,"countries/(.*[a-z])\\.")[,2])
# Adding NAs for missing profiles
for (i in missingCountryLinks){
playersCountries <- append(playersCountries, NA, after = i)
}
# Adding Player countries to main table
players <- add_column(players,as.factor(playersCountries), .after = "Name")
names(players)[4] <- c("Country")
str(players)
# Add year to main table
players <- add_column(players, y, .before = "Rank")
names(players)[1] <- c("Year")
str(players)
playersAllYears <- rbind(playersAllYears, players)
}
#Loading libraries
library("XML")
library("rvest")
library("tidyverse")
library("stringr")
library("ggplot2")
# Initiate variable
playersAllYears <- data.frame(
Year = numeric(),
Rank = numeric(),
ID = factor(),
Name = character(),
Country = factor(),
OfficialMoney = numeric(),
EventsPlayed = numeric()
)
# year <- seq(from = 2018, to = 2000, by = -1)
y <- 2016
url <- c('http://www.lpga.com/statistics/money/official-money?year=')
# Downloading html
fileName <- paste("golf",y,".html")
fileNameClean <- paste("golfClean",y,".html")
path <- file.path("~","GitHub","ieseDataSciProject",fileName)
# Players scraped in table
colTypes <- c("numeric","character","character","numeric")
players <- readHTMLTable(paste(url,y), colClasses = colTypes, stringsAsFactors = FALSE, as.data.frame = TRUE, which = 1)
names(players)[3:4] <- c("OfficialMoney", "EventsPlayed")
str(players)
# Converting official money to numeric
players$`OfficialMoney`<- as.numeric(gsub("\\$|,","",players$`OfficialMoney`))
str(players)
?str_detect
str_detect(read_html(paste(url,y)),"(/players/.*) ")
paste(url,y)
?paste
# year <- seq(from = 2018, to = 2000, by = -1)
y <- c("2016")
paste(url,y)
read_html(paste(url,y))
?read_html
#Loading libraries
library("XML")
library("rvest")
library("tidyverse")
library("stringr")
library("ggplot2")
# Initiate variable
playersAllYears <- data.frame(
Year = numeric(),
Rank = numeric(),
ID = factor(),
Name = character(),
Country = factor(),
OfficialMoney = numeric(),
EventsPlayed = numeric()
)
# year <- seq(from = 2018, to = 2000, by = -1)
y <- c("2016")
url <- c('http://www.lpga.com/statistics/money/official-money?year=')
# Downloading html
fileName <- paste("golf",y,".html", sep = "")
fileNameClean <- paste("golfClean",y,".html", sep= "")
path <- file.path("~","GitHub","ieseDataSciProject",fileName)
# Players scraped in table
colTypes <- c("numeric","character","character","numeric")
players <- readHTMLTable(paste(url,y, sep = ""), colClasses = colTypes, stringsAsFactors = FALSE, as.data.frame = TRUE, which = 1)
names(players)[3:4] <- c("OfficialMoney", "EventsPlayed")
str(players)
# Converting official money to numeric
players$`OfficialMoney`<- as.numeric(gsub("\\$|,","",players$`OfficialMoney`))
str(players)
read_html(paste(url,y))
read_html(paste(url,y,sep=""))
str_detect(read_html(paste(url,y, sep = "")),"(/players/.*) ")
source('~/GitHub/ieseDataSciProject/golf.r')
source('~/GitHub/ieseDataSciProject/golf.r')
source('~/GitHub/ieseDataSciProject/golf.r')
source('~/GitHub/ieseDataSciProject/golf.r')
if (str_detect(read_html(tableURL),"(/players/.*) (.*/overview)")){
print(paste("Errors identified in table of year", y))
fileName <- paste("golf",y,".html", sep = "")
fileNameClean <- paste("golfClean",y,".html", sep= "")
path <- file.path("~","GitHub","ieseDataSciProject",fileName)
# Identifying errors in player profile links, fixing, and writing in new file
download.file(tableURL, destfile = path)
cleanHTML <- sapply(read_lines(fileName),str_replace, "(/players/.*) ","\\1-")
write(cleanHTML, fileNameClean)
# Reading HTML nodes
golfHTML <- read_html(fileNameClean)
} else {
golfHTML <- read_html(tableURL)
}
str_detect(read_html(tableURL),"(/players/.*) (.*/overview)")
str_detect(read_html(tableURL),"/players/.* .*/overview")
str_detect(read_html(tableURL),"/players/.* .*overview")
str_detect(read_html(tableURL),"/players/.* ")
str_detect(read_html(tableURL),"/players/.* overview")
str_detect(read_html(tableURL),"/players/.* .*overview")
fileName <- paste("golf",y,".html", sep = "")
fileNameClean <- paste("golfClean",y,".html", sep= "")
path <- file.path("~","GitHub","ieseDataSciProject",fileName)
# Identifying errors in player profile links, fixing, and writing in new file
download.file(tableURL, destfile = path)
str_detect(read_html(tableURL),"/players/.* +.*overview")
str_detect(read_html(tableURL),"/players/.* ")
tableURL
str_detect(read_html(tableURL),"/players/.* ")
read_html(tableURL)
read_html(tableURL)[2]
str_detect(read_html(tableURL),"/players/.* ")
str_detect(read_html(tableURL),"/players/.*")
str_detect(read_html(tableURL),"/players/.* ")
str_detect(read_html(tableURL),"/plaers/.* ")
str_detect(read_html(tableURL),"/players/.* ")
str_detect(read_html(tableURL),"/players/.* \\.*overview")
str_detect(read_html(tableURL),"/players/.* \.*overview")
str_detect(read_html(tableURL),"/players/.* \\.*overview")
str_detect(read_html(tableURL),"/players/.* .*overview")
str_detect(read_html(tableURL),"/players/.* *overview")
str_detect(read_html(tableURL),"/players/.* */overview")
str_detect(read_html(paste(url,2010)),"/players/.* */overview")
str_detect(read_html(paste(url,c("2010"))),"/players/.* */overview")
str_detect(read_html(paste(url,"2010")),"/players/.* */overview")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.* */overview")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.* */overview")
str_detect(read_html(paste(url,"2000", sep="")),"/players/.* */overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.* */overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.* *0-9/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.* *[0-9]+/overview")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.* *[0-9]+/overview")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.* [a-z0-9]+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.* [a-z0-9]+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.* [a-z0-9]*/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.* [a-z0-9]/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.* [a-z0-9]+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.* [0-9]+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.* *[0-9]+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.* [a-z]+/[0-9]+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.* [a-z]*/[0-9]+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.* [a-z]*/[0-9]*/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.* [a-z]+/[0-9]{5}/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.* */[0-9]{5}/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.* [^\n]")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.* [^\n]")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.* [^\n]/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.* [^\n]/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.* .*/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ .+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ ")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.+ ")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.+ .+")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ .+")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ .+")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ .+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/[.+] [.+]/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/[.+ .+]/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ .+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+[ ].+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ .+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+/[0-9]{5}/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/\w+/[0-9]{5}/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/\\w+/[0-9]{5}/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/\\w+ \\w+/[0-9]{5}/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/\\w+[a-z]{3}/[0-9]{5}/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/\\w+ [a-z]{3}/[0-9]{5}/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+\\h.+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+\\h.+")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.+\\h.+")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.+\\h")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+\\h")
str_detect(read_html(paste(url,"2018", sep="")),"/players/\\h")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+\\h")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.+\\h")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.+\\h.+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+\\h.+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+[\\h].+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+\\h[a-z]+/[0-9]+/overview")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ [a-z]+/[0-9]+/overview")
read_html(paste(url,"2018", sep=""))
html_nodes(read_html(paste(url,"2018", sep="")))
html_nodes(read_html(paste(url,"2018", sep="")), xpath = "//tbody/tr/td[contains(@class, 'table-content left')]/a")
head(html_nodes(read_html(paste(url,"2018", sep="")), xpath = "//tbody/tr/td[contains(@class, 'table-content left')]/a"), n=50)
html_nodes(read_html(paste(url,"2018", sep="")), xpath = "//tbody/tr/td[contains(@class, 'table-content left')]/a")[30]
html_nodes(read_html(paste(url,"2018", sep="")), xpath = "//tbody/tr/td[contains(@class, 'table-content left')]/a")[31]
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ ")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.+ ")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ .+>")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.+ .+>")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ .+>")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ hsu>")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.+ hsu>")
html_nodes(read_html(paste(url,"2018", sep="")), xpath = "//tbody/tr/td[contains(@class, 'table-content left')]/a")[31]
str_detect(read_html(paste(url,"2010", sep="")),"/players/.+ hsu>")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.+ [a-z]>")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ [a-z]>")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ [a-z]{1}>")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ [a-z]{3}>")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ [a-z]+>")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.+ [a-z]+>")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.+ .+>")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.+ .*>")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.+ [a-z0-9/]*>")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ [a-z0-9/]*>")
str_detect(read_html(paste(url,"2018", sep="")),"/players/.+ [a-z0-9/-]*>")
str_detect(read_html(paste(url,"2010", sep="")),"/players/.+ [a-z0-9/-]*>")
source('~/GitHub/ieseDataSciProject/golf.r')
source('~/GitHub/ieseDataSciProject/golf.r')
#fileName <- paste("golf",y,".html", sep = "")
#fileNameClean <- paste("golfClean",y,".html", sep= "")
#path <- file.path("~","GitHub","ieseDataSciProject",fileName)
#download.file(tableURL, destfile = path)
cleanHTML <- sapply(read_html(tableURL),str_replace, "(/players/.+) ([a-z0-9/-]*>)","\\1-\\2")
cleanHTML <- sapply(read_html(tableURL),str_replace, "(/players/.+) ([a-z0-9/-]*>)","\\1-\\2")
cleanHTML <- sapply(read_html(tableURL),str_replace, "(/players/.+) ","\\1-")
cleanHTML <- str_replace_all(read_html(tableURL), "(/players/.+) ([a-z0-9/-]*>)","\\1-\\2")
read_html(cleanHTML)
source('~/GitHub/ieseDataSciProject/golf.r')
View(playersAllYears)
View(playersAllYears)
View(playersAllYears)
source('~/GitHub/ieseDataSciProject/golf.r')
View(playersAllYears)
#Loading libraries
library("XML")
library("rvest")
library("tidyverse")
library("stringr")
library("ggplot2")
# Initiate variable
playersAllYears <- data.frame(
Year = numeric(),
Rank = numeric(),
ID = factor(),
Name = character(),
Country = factor(),
OfficialMoney = numeric(),
EventsPlayed = numeric()
)
#year <- seq(from = 2018, to = 2000, by = -1)
y <- c("2018")
url <- c('http://www.lpga.com/statistics/money/official-money?year=')
#for (y in year) {
print(paste("Processing data for year",y))
# Defining working URL
tableURL <- paste(url,y,sep = "")
# Players scraped in table
colTypes <- c("numeric","character","character","numeric")
players <- readHTMLTable(tableURL, colClasses = colTypes, stringsAsFactors = FALSE, as.data.frame = TRUE, which = 1)
names(players)[3:4] <- c("OfficialMoney", "EventsPlayed")
# Converting official money to numeric
players$`OfficialMoney`<- as.numeric(gsub("\\$|,","",players$`OfficialMoney`))
# Identifying errors in player profile links, fixing, and writing in new file
if (str_detect(read_html(tableURL),"/players/.+ [a-z0-9/-]*>")){
print(paste("Errors identified in table of year", y,"... Fixing!"))
cleanHTML <- str_replace_all(read_html(tableURL), "(/players/.+) ","\\1-")
# Reading HTML nodes
golfHTML <- read_html(cleanHTML)
} else {
golfHTML <- read_html(tableURL)
}
cleanHTML
golfPlayers <- html_nodes(golfHTML, xpath = "//tbody/tr/td[contains(@class, 'table-content left')]/a")
# Identifying missing link attributes
missingProfileLinks <- which(sapply(html_attrs(golfPlayers),is_empty)) -1
# Extracting player ID
golfPlayersURL <- unlist(html_attrs(golfPlayers))
golfPlayersURL
?read_html
#Loading libraries
library("XML")
library("rvest")
library("tidyverse")
library("stringr")
library("ggplot2")
# Initiate variable
playersAllYears <- data.frame(
Year = numeric(),
Rank = numeric(),
ID = factor(),
Name = character(),
Country = factor(),
OfficialMoney = numeric(),
EventsPlayed = numeric()
)
#year <- seq(from = 2018, to = 2000, by = -1)
y <- c("2018")
url <- c('http://www.lpga.com/statistics/money/official-money?year=')
#for (y in year) {
print(paste("Processing data for year",y))
# Defining working URL
tableURL <- paste(url,y,sep = "")
# Players scraped in table
colTypes <- c("numeric","character","character","numeric")
players <- readHTMLTable(tableURL, colClasses = colTypes, stringsAsFactors = FALSE, as.data.frame = TRUE, which = 1)
names(players)[3:4] <- c("OfficialMoney", "EventsPlayed")
# Converting official money to numeric
players$`OfficialMoney`<- as.numeric(gsub("\\$|,","",players$`OfficialMoney`))
# Identifying errors in player profile links, fixing, and writing in new file
if (str_detect(read_html(tableURL),"/players/.+ [a-z0-9/-]*>")){
print(paste("Errors identified in table of year", y,"... Fixing!"))
fileName <- paste("golf",y,".html")
fileNameClean <- paste("golfClean",y,".html")
path <- file.path("~","GitHub","ieseDataSciProject",fileName)
download.file(paste(url,y), destfile = path)
cleanHTML <- sapply(read_lines(fileName),str_replace, "(/players/.*) ","\\1-")
# Reading HTML nodes
golfHTML <- read_html(cleanHTML)
} else {
golfHTML <- read_html(tableURL)
}
#Loading libraries
library("XML")
library("rvest")
library("tidyverse")
library("stringr")
library("ggplot2")
# Initiate variable
playersAllYears <- data.frame(
Year = numeric(),
Rank = numeric(),
ID = factor(),
Name = character(),
Country = factor(),
OfficialMoney = numeric(),
EventsPlayed = numeric()
)
#year <- seq(from = 2018, to = 2000, by = -1)
y <- c("2018")
url <- c('http://www.lpga.com/statistics/money/official-money?year=')
#for (y in year) {
print(paste("Processing data for year",y))
# Defining working URL
tableURL <- paste(url,y,sep = "")
# Players scraped in table
colTypes <- c("numeric","character","character","numeric")
players <- readHTMLTable(tableURL, colClasses = colTypes, stringsAsFactors = FALSE, as.data.frame = TRUE, which = 1)
names(players)[3:4] <- c("OfficialMoney", "EventsPlayed")
# Converting official money to numeric
players$`OfficialMoney`<- as.numeric(gsub("\\$|,","",players$`OfficialMoney`))
# Identifying errors in player profile links, fixing, and writing in new file
if (str_detect(read_html(tableURL),"/players/.+ [a-z0-9/-]*>")){
print(paste("Errors identified in table of year", y,"... Fixing!"))
cleanHTML <- str_replace_all(read_html(tableURL), "(/players/.+) ","\\1-")
# Reading HTML nodes
golfHTML <- read_html(cleanHTML)
} else {
golfHTML <- read_html(tableURL)
}
golfPlayers <- html_nodes(golfHTML, xpath = "//tbody/tr/td[contains(@class, 'table-content left')]/a")
# Identifying missing link attributes
missingProfileLinks <- which(sapply(html_attrs(golfPlayers),is_empty)) -1
# Extracting player ID
golfPlayersURL <- unlist(html_attrs(golfPlayers))
golfPlayersURL
cleanHTML <- str_replace_all(readLines(tableURL), "(/players/.+) ","\\1-")
# Reading HTML nodes
golfHTML <- read_html(cleanHTML)
readLines(tableURL)
cleanHTML <- sapply(readLines(tableURL),str_replace, "(/players/.*) ","\\1-")
# Reading HTML nodes
golfHTML <- read_html(cleanHTML)
cleanHTML <- tohtml(sapply(readLines(tableURL),str_replace, "(/players/.*) ","\\1-"))
cleanHTML <- paste(sapply(readLines(tableURL),str_replace, "(/players/.*) ","\\1-"),sep="")
# Reading HTML nodes
golfHTML <- read_html(cleanHTML)
# Identifying errors in player profile links, fixing, and writing in new file
if (str_detect(read_html(tableURL),"/players/.+ [a-z0-9/-]*>")){
print(paste("Errors identified in table of year", y,"... Fixing!"))
cleanHTML <- str_replace_all(read_html(tableURL), "(/players/.+) ","\\1-")
# Reading HTML nodes
golfHTML <- read_html(cleanHTML)
} else {
golfHTML <- read_html(tableURL)
}
cleanHTML <- paste(sapply(readLines(tableURL),str_replace, "(/players/.*) ","\\1-"),collapse="")
# Reading HTML nodes
golfHTML <- read_html(cleanHTML)
#Loading libraries
library("XML")
library("rvest")
library("tidyverse")
library("stringr")
library("ggplot2")
# Initiate variable
playersAllYears <- data.frame(
Year = numeric(),
Rank = numeric(),
ID = factor(),
Name = character(),
Country = factor(),
OfficialMoney = numeric(),
EventsPlayed = numeric()
)
#year <- seq(from = 2018, to = 2000, by = -1)
y <- c("2018")
url <- c('http://www.lpga.com/statistics/money/official-money?year=')
#for (y in year) {
print(paste("Processing data for year",y))
# Defining working URL
tableURL <- paste(url,y,sep = "")
# Players scraped in table
colTypes <- c("numeric","character","character","numeric")
players <- readHTMLTable(tableURL, colClasses = colTypes, stringsAsFactors = FALSE, as.data.frame = TRUE, which = 1)
names(players)[3:4] <- c("OfficialMoney", "EventsPlayed")
# Converting official money to numeric
players$`OfficialMoney`<- as.numeric(gsub("\\$|,","",players$`OfficialMoney`))
# Identifying errors in player profile links, fixing, and writing in new file
if (str_detect(read_html(tableURL),"/players/.+ [a-z0-9/-]*>")){
print(paste("Errors identified in table of year", y,"... Fixing!"))
cleanHTML <- paste(sapply(readLines(tableURL),str_replace, "(/players/.*) ","\\1-"),collapse="")
# Reading HTML nodes
golfHTML <- read_html(cleanHTML)
} else {
golfHTML <- read_html(tableURL)
}
golfPlayers <- html_nodes(golfHTML, xpath = "//tbody/tr/td[contains(@class, 'table-content left')]/a")
# Identifying missing link attributes
missingProfileLinks <- which(sapply(html_attrs(golfPlayers),is_empty)) -1
# Extracting player ID
golfPlayersURL <- unlist(html_attrs(golfPlayers))
golfPlayersURL
cleanHTML <- paste(sapply(readLines(tableURL),str_replace, "(/players/.+) ([a-z0-9/-]*>)","\\1-\\2"),collapse="")
# Reading HTML nodes
golfHTML <- read_html(cleanHTML)
golfPlayers <- html_nodes(golfHTML, xpath = "//tbody/tr/td[contains(@class, 'table-content left')]/a")
# Identifying missing link attributes
missingProfileLinks <- which(sapply(html_attrs(golfPlayers),is_empty)) -1
# Extracting player ID
golfPlayersURL <- unlist(html_attrs(golfPlayers))
golfPlayersURL
source('~/GitHub/ieseDataSciProject/golf.r')
